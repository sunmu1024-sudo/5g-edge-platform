"""
æ··åˆå­˜å‚¨ç®¡ç†å™¨
- SQLite: ä¸»è¦å­˜å‚¨ï¼ˆå®‰å…¨ï¼‰
- JSON: å¤‡ä»½å’Œè°ƒè¯•ï¼ˆä¾¿åˆ©ï¼‰  
- å†…å­˜: å®æ—¶ç¼“å­˜ï¼ˆæ€§èƒ½ï¼‰
"""

import sqlite3
import json
import os
import threading
from datetime import datetime, timedelta
import logging
import shutil

class HybridStorageManager:
    """
    æ··åˆå­˜å‚¨ç®¡ç†å™¨
    - SQLite: ä¸»è¦å­˜å‚¨ï¼ˆå®‰å…¨ï¼‰
    - JSON: å¤‡ä»½å’Œè°ƒè¯•ï¼ˆä¾¿åˆ©ï¼‰  
    - å†…å­˜: å®æ—¶ç¼“å­˜ï¼ˆæ€§èƒ½ï¼‰
    """
    
    def __init__(self, db_path='sensor_data.db', json_dir='data_backup'):
        self.db_path = db_path
        self.json_dir = json_dir
        self.memory_cache = {}
        self.lock = threading.RLock()
        self.sync_thread = None
        self.running = True
        
        # åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨
        self._init_sqlite()
        self._init_json_backup()
        self._start_sync_thread()
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger('HybridStorage')
    
    def _init_sqlite(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # å¯ç”¨WALæ¨¡å¼ï¼ˆå†™æ—¶å¤åˆ¶ï¼Œæé«˜å¹¶å‘æ€§ï¼‰
            cursor.execute('PRAGMA journal_mode=WAL')
            
            # ä¼ æ„Ÿå™¨è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sensors (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    type TEXT NOT NULL,
                    location TEXT,
                    unit TEXT,
                    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_seen TIMESTAMP
                )
            ''')
            
            # ä¼ æ„Ÿå™¨æ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sensor_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    sensor_id TEXT NOT NULL,
                    value REAL,
                    status TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    data_quality INTEGER DEFAULT 1,
                    FOREIGN KEY (sensor_id) REFERENCES sensors (id)
                )
            ''')
            
            # ç³»ç»Ÿäº‹ä»¶è¡¨ï¼ˆå®¡è®¡æ—¥å¿—ï¼‰
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS system_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    event_type TEXT NOT NULL,
                    event_data TEXT,
                    severity TEXT DEFAULT 'info',
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_sensor_data_composite 
                ON sensor_data(sensor_id, timestamp DESC)
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info("âœ… SQLiteæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ SQLiteåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_json_backup(self):
        """åˆå§‹åŒ–JSONå¤‡ä»½ç›®å½•"""
        try:
            if not os.path.exists(self.json_dir):
                os.makedirs(self.json_dir)
                self.logger.info(f"âœ… JSONå¤‡ä»½ç›®å½•åˆ›å»º: {self.json_dir}")
        except Exception as e:
            self.logger.error(f"âŒ JSONå¤‡ä»½ç›®å½•åˆ›å»ºå¤±è´¥: {e}")
    
    def save_sensor_reading(self, sensor_id, value, status='online', metadata=None):
        """
        ä¿å­˜ä¼ æ„Ÿå™¨è¯»æ•° - ä¸‰çº§å­˜å‚¨
        1. å†…å­˜ç¼“å­˜ï¼ˆç«‹å³ï¼‰
        2. SQLiteæ•°æ®åº“ï¼ˆç«‹å³ï¼‰  
        3. JSONå¤‡ä»½ï¼ˆå¼‚æ­¥ï¼‰
        """
        timestamp = datetime.now()
        
        try:
            with self.lock:
                # 1. æ›´æ–°å†…å­˜ç¼“å­˜
                self._update_memory_cache(sensor_id, value, status, timestamp)
                
                # 2. ä¿å­˜åˆ°SQLiteï¼ˆå¸¦äº‹åŠ¡ï¼‰
                self._save_to_sqlite(sensor_id, value, status, timestamp, metadata)
                
                # 3. å¼‚æ­¥JSONå¤‡ä»½ï¼ˆä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰
                threading.Thread(
                    target=self._async_json_backup,
                    args=(sensor_id, value, status, timestamp),
                    daemon=True
                ).start()
                
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ä¼ æ„Ÿå™¨æ•°æ®å¤±è´¥ {sensor_id}: {e}")
            self._log_system_event('storage_error', f'Sensor {sensor_id} save failed: {e}', 'error')
            return False
    
    def _update_memory_cache(self, sensor_id, value, status, timestamp):
        """æ›´æ–°å†…å­˜ç¼“å­˜"""
        cache_key = f"{sensor_id}_latest"
        self.memory_cache[cache_key] = {
            'value': value,
            'status': status,
            'timestamp': timestamp.isoformat(),
            'cached_at': datetime.now().isoformat()
        }
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.memory_cache) > 1000:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.memory_cache))
            del self.memory_cache[oldest_key]
    
    def _save_to_sqlite(self, sensor_id, value, status, timestamp, metadata):
        """ä¿å­˜åˆ°SQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        try:
            cursor = conn.cursor()
            
            # å¼€å§‹äº‹åŠ¡
            cursor.execute('BEGIN TRANSACTION')
            
            # æ’å…¥ä¼ æ„Ÿå™¨æ•°æ®
            cursor.execute('''
                INSERT INTO sensor_data (sensor_id, value, status, timestamp)
                VALUES (?, ?, ?, ?)
            ''', (sensor_id, value, status, timestamp))
            
            # æ›´æ–°ä¼ æ„Ÿå™¨æœ€åæ´»è·ƒæ—¶é—´
            cursor.execute('''
                INSERT OR REPLACE INTO sensors (id, name, type, location, unit, last_seen)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                sensor_id,
                metadata.get('name', 'Unknown') if metadata else 'Unknown',
                metadata.get('type', 'unknown') if metadata else 'unknown', 
                metadata.get('location', 'Unknown') if metadata else 'Unknown',
                metadata.get('unit', '') if metadata else '',
                timestamp
            ))
            
            # æäº¤äº‹åŠ¡
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()
    
    def _async_json_backup(self, sensor_id, value, status, timestamp):
        """å¼‚æ­¥JSONå¤‡ä»½"""
        try:
            backup_file = os.path.join(self.json_dir, f'{sensor_id}_backup.json')
            backup_data = {
                'sensor_id': sensor_id,
                'value': value,
                'status': status,
                'timestamp': timestamp.isoformat(),
                'backup_time': datetime.now().isoformat()
            }
            
            # è¯»å–ç°æœ‰å¤‡ä»½æ•°æ®
            existing_data = []
            if os.path.exists(backup_file):
                try:
                    with open(backup_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except:
                    existing_data = []
            
            # æ·»åŠ æ–°æ•°æ®ï¼ˆé™åˆ¶å¤§å°ï¼‰
            existing_data.append(backup_data)
            if len(existing_data) > 1000:  # æœ€å¤š1000æ¡å¤‡ä»½
                existing_data = existing_data[-1000:]
            
            # å†™å…¥å¤‡ä»½æ–‡ä»¶
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ JSONå¤‡ä»½å¤±è´¥ {sensor_id}: {e}")
    
    def get_latest_readings(self, use_cache=True):
        """è·å–æœ€æ–°è¯»æ•° - ä¼˜å…ˆä½¿ç”¨ç¼“å­˜"""
        if use_cache and self.memory_cache:
            # ä»å†…å­˜ç¼“å­˜æ„å»ºç»“æœ
            result = {}
            for key, data in self.memory_cache.items():
                if key.endswith('_latest'):
                    sensor_id = key.replace('_latest', '')
                    result[sensor_id] = data
            return result
        
        # ç¼“å­˜ä¸å¯ç”¨ï¼Œä»æ•°æ®åº“æŸ¥è¯¢
        return self._get_latest_from_sqlite()
    
    def _get_latest_from_sqlite(self):
        """ä»SQLiteè·å–æœ€æ–°æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        try:
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT s.id, s.name, s.type, s.location, s.unit,
                       sd.value, sd.status, sd.timestamp
                FROM sensors s
                JOIN sensor_data sd ON s.id = sd.sensor_id
                WHERE sd.timestamp = (
                    SELECT MAX(timestamp) 
                    FROM sensor_data 
                    WHERE sensor_id = s.id
                )
            ''')
            
            results = {}
            for row in cursor.fetchall():
                results[row[0]] = {
                    'name': row[1],
                    'type': row[2],
                    'location': row[3],
                    'unit': row[4],
                    'value': row[5],
                    'status': row[6],
                    'timestamp': row[7]
                }
            
            return results
            
        finally:
            conn.close()
    
    def get_sensor_history(self, sensor_id, hours=24, source='auto'):
        """è·å–ä¼ æ„Ÿå™¨å†å²æ•°æ®"""
        if source == 'auto':
            # è‡ªåŠ¨é€‰æ‹©ï¼šæœ€è¿‘æ•°æ®ç”¨ç¼“å­˜ï¼Œå†å²æ•°æ®ç”¨æ•°æ®åº“
            if hours <= 1:  # 1å°æ—¶å†…æ•°æ®å°è¯•ä»ç¼“å­˜è·å–
                cached = self._get_recent_from_cache(sensor_id, hours)
                if cached:
                    return cached
            
        # ä»æ•°æ®åº“è·å–
        return self._get_history_from_sqlite(sensor_id, hours)
    
    def _get_recent_from_cache(self, sensor_id, hours):
        """ä»ç¼“å­˜è·å–è¿‘æœŸæ•°æ®"""
        cache_key = f"{sensor_id}_latest"
        if cache_key in self.memory_cache:
            data = self.memory_cache[cache_key]
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
            data_time = datetime.fromisoformat(data['timestamp'])
            if (datetime.now() - data_time).total_seconds() <= hours * 3600:
                return [data]
        return []
    
    def _get_history_from_sqlite(self, sensor_id, hours):
        """ä»SQLiteè·å–å†å²æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        try:
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT timestamp, value, status 
                FROM sensor_data 
                WHERE sensor_id = ? AND timestamp >= datetime('now', ?)
                ORDER BY timestamp DESC
                LIMIT 1000
            ''', (sensor_id, f'-{hours} hours'))
            
            return [
                {
                    'timestamp': row[0],
                    'value': row[1], 
                    'status': row[2]
                }
                for row in cursor.fetchall()
            ]
            
        finally:
            conn.close()
    
    def _log_system_event(self, event_type, event_data, severity='info'):
        """è®°å½•ç³»ç»Ÿäº‹ä»¶"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO system_events (event_type, event_data, severity)
                VALUES (?, ?, ?)
            ''', (event_type, json.dumps(event_data), severity))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"âŒ è®°å½•ç³»ç»Ÿäº‹ä»¶å¤±è´¥: {e}")
    
    def _start_sync_thread(self):
        """å¯åŠ¨æ•°æ®åŒæ­¥çº¿ç¨‹"""
        def sync_worker():
            while self.running:
                try:
                    self._sync_json_to_sqlite()
                    threading.Event().wait(30)  # æ¯30ç§’åŒæ­¥ä¸€æ¬¡
                except Exception as e:
                    self.logger.error(f"âŒ æ•°æ®åŒæ­¥å¤±è´¥: {e}")
                    threading.Event().wait(60)  # å‡ºé”™æ—¶ç­‰å¾…æ›´ä¹…
        
        self.sync_thread = threading.Thread(target=sync_worker, daemon=True)
        self.sync_thread.start()
    
    def _sync_json_to_sqlite(self):
        """å°†JSONå¤‡ä»½æ•°æ®åŒæ­¥åˆ°SQLite"""
        # å®ç°JSONåˆ°SQLiteçš„æ•°æ®æ¢å¤åŒæ­¥
        # ç”¨äºç¾éš¾æ¢å¤åœºæ™¯
        pass
    
    def backup_database(self, backup_dir='backups'):
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)
            
            backup_file = os.path.join(
                backup_dir, 
                f'sensor_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.db'
            )
            
            # SQLiteå¤‡ä»½
            conn = sqlite3.connect(self.db_path)
            backup_conn = sqlite3.connect(backup_file)
            
            conn.backup(backup_conn)
            backup_conn.close()
            conn.close()
            
            self.logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}")
            return backup_file
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return None
    
    def get_system_stats(self, days=7):
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        try:
            cursor = conn.cursor()
            
            # ä¼ æ„Ÿå™¨æ•°é‡ç»Ÿè®¡
            cursor.execute('SELECT COUNT(*) FROM sensors')
            total_sensors = cursor.fetchone()[0]
            
            # åœ¨çº¿ä¼ æ„Ÿå™¨æ•°é‡ï¼ˆæœ€è¿‘5åˆ†é’Ÿæœ‰æ•°æ®çš„ï¼‰
            cursor.execute('''
                SELECT COUNT(DISTINCT sensor_id) 
                FROM sensor_data 
                WHERE timestamp >= datetime('now', '-5 minutes')
            ''')
            online_sensors = cursor.fetchone()[0]
            
            # æ•°æ®ç‚¹æ€»æ•°
            cursor.execute('SELECT COUNT(*) FROM sensor_data')
            total_readings = cursor.fetchone()[0]
            
            # ä»Šæ—¥æ•°æ®é‡
            cursor.execute('''
                SELECT COUNT(*) 
                FROM sensor_data 
                WHERE date(timestamp) = date('now')
            ''')
            today_readings = cursor.fetchone()[0]
            
            return {
                'total_sensors': total_sensors,
                'online_sensors': online_sensors,
                'total_readings': total_readings,
                'today_readings': today_readings,
                'storage_size_mb': round(os.path.getsize(self.db_path) / (1024*1024), 2)
            }
            
        finally:
            conn.close()
    
    def close(self):
        """å…³é—­å­˜å‚¨ç®¡ç†å™¨"""
        self.running = False
        if self.sync_thread:
            self.sync_thread.join(timeout=5)
        
        # æ‰§è¡Œæœ€ç»ˆå¤‡ä»½
        self.backup_database()
        self.logger.info("ğŸ”’ æ··åˆå­˜å‚¨ç®¡ç†å™¨å·²å…³é—­")

# æµ‹è¯•å‡½æ•°
def test_hybrid_storage():
    """æµ‹è¯•æ··åˆå­˜å‚¨"""
    print("ğŸ§ª æµ‹è¯•æ··åˆå­˜å‚¨ç®¡ç†å™¨...")
    
    storage = HybridStorageManager()
    
    # æµ‹è¯•ä¿å­˜æ•°æ®
    test_data = [
        ('temp_001', 22.5, 'online', {'name': 'æ¸©åº¦ä¼ æ„Ÿå™¨1', 'type': 'temperature', 'location': 'AåŒº', 'unit': 'Â°C'}),
        ('humidity_001', 55.0, 'online', {'name': 'æ¹¿åº¦ä¼ æ„Ÿå™¨1', 'type': 'humidity', 'location': 'AåŒº', 'unit': '%'}),
        ('light_001', 450, 'online', {'name': 'å…‰ç…§ä¼ æ„Ÿå™¨1', 'type': 'light', 'location': 'BåŒº', 'unit': 'lux'})
    ]
    
    for sensor_id, value, status, metadata in test_data:
        success = storage.save_sensor_reading(sensor_id, value, status, metadata)
        print(f"ä¿å­˜ {sensor_id}: {'âœ…' if success else 'âŒ'}")
    
    # æµ‹è¯•è¯»å–æ•°æ®
    latest = storage.get_latest_readings()
    print(f"æœ€æ–°è¯»æ•°: {len(latest)} ä¸ªä¼ æ„Ÿå™¨")
    
    stats = storage.get_system_stats()
    print(f"ç³»ç»Ÿç»Ÿè®¡: {stats}")
    
    storage.close()
    print("âœ… æ··åˆå­˜å‚¨æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_hybrid_storage()